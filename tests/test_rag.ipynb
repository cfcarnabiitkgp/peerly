{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG System Testing Notebook\n",
    "\n",
    "This notebook allows you to test and verify the RAG (Retrieval-Augmented Generation) system for both Clarity and Rigor agents.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. Qdrant running: `docker run -p 6333:6333 qdrant/qdrant`\n",
    "2. Documents embedded: `python scripts/embed_documents.py --all`\n",
    "3. Environment variables set (`.env` file with `OPENAI_API_KEY`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nfrom pathlib import Path\n\n# Add parent directory to path\nsys.path.append(str(Path().absolute().parent))\n\nimport time\nfrom qdrant_client import QdrantClient\nfrom app.rag.caching import create_cached_embeddings\nfrom app.rag.config import clarity_rag_config, rigor_rag_config\nfrom app.rag.rag_service import RAGService\nfrom app.agents.rag_nodes import ClarityRAGNode, RigorRAGNode\nfrom app.models.schemas import Section\n\nprint(\"âœ“ Imports successful\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Check Qdrant Connection and Collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Qdrant\n",
    "qdrant_client = QdrantClient(url=\"http://localhost:6333\")\n",
    "\n",
    "try:\n",
    "    # Get all collections\n",
    "    collections = qdrant_client.get_collections()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"QDRANT COLLECTIONS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if not collections.collections:\n",
    "        print(\"âš ï¸  No collections found!\")\n",
    "        print(\"   Run: python scripts/embed_documents.py --all\")\n",
    "    else:\n",
    "        for collection in collections.collections:\n",
    "            info = qdrant_client.get_collection(collection.name)\n",
    "            print(f\"\\nðŸ“š Collection: {collection.name}\")\n",
    "            print(f\"   Points: {info.points_count}\")\n",
    "            print(f\"   Vectors: {info.config.params.vectors}\")\n",
    "            \n",
    "    print(\"\\nâœ“ Qdrant connection successful\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Error connecting to Qdrant: {e}\")\n",
    "    print(\"   Make sure Qdrant is running: docker run -p 6333:6333 qdrant/qdrant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test Embedding Cache\n",
    "\n",
    "Verify that caching is working properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TESTING EMBEDDING CACHE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create cached embeddings\n",
    "embeddings = create_cached_embeddings(namespace=\"test\")\n",
    "\n",
    "# Test texts\n",
    "test_texts = [\n",
    "    \"Mathematical proofs require rigorous validation.\",\n",
    "    \"Clear writing is essential for mathematical papers.\",\n",
    "    \"Theorems should be stated precisely.\"\n",
    "] * 10  # 30 texts total\n",
    "\n",
    "# First run (no cache)\n",
    "print(f\"\\nðŸ”„ First run (embedding {len(test_texts)} texts)...\")\n",
    "start = time.time()\n",
    "result1 = embeddings.embed_documents(test_texts)\n",
    "first_time = time.time() - start\n",
    "print(f\"   Time: {first_time:.3f}s\")\n",
    "print(f\"   Embeddings shape: {len(result1)} vectors of dimension {len(result1[0])}\")\n",
    "\n",
    "# Second run (cache hit)\n",
    "print(f\"\\nâš¡ Second run (using cache)...\")\n",
    "start = time.time()\n",
    "result2 = embeddings.embed_documents(test_texts)\n",
    "cache_time = time.time() - start\n",
    "print(f\"   Time: {cache_time:.3f}s\")\n",
    "print(f\"   Speedup: {first_time/cache_time:.1f}x faster\")\n",
    "\n",
    "# Verify results are identical\n",
    "assert result1 == result2, \"Cache results should match!\"\n",
    "print(\"\\nâœ“ Cache working correctly!\")\n",
    "\n",
    "# Show cache location\n",
    "cache_dir = Path(\"app/embedding_cache/test\")\n",
    "if cache_dir.exists():\n",
    "    cache_files = list(cache_dir.glob(\"*\"))\n",
    "    print(f\"\\nðŸ“‚ Cache location: {cache_dir}\")\n",
    "    print(f\"   Files: {len(cache_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Clarity Agent RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TESTING CLARITY AGENT RAG\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create RAG service for clarity agent\n",
    "clarity_service = RAGService(config=clarity_rag_config, qdrant_client=qdrant_client)\n",
    "\n",
    "# Test queries for clarity\n",
    "clarity_queries = [\n",
    "    \"how to write clear mathematical statements\",\n",
    "    \"avoiding complex sentences in technical writing\",\n",
    "    \"defining jargon and technical terms\",\n",
    "    \"improving readability of proofs\"\n",
    "]\n",
    "\n",
    "for i, query in enumerate(clarity_queries, 1):\n",
    "    print(f\"\\n{'â”€'*60}\")\n",
    "    print(f\"Query {i}: {query}\")\n",
    "    print(f\"{'â”€'*60}\")\n",
    "    \n",
    "    try:\n",
    "        # Retrieve documents\n",
    "        start = time.time()\n",
    "        results = clarity_service.retrieve(query, top_k=3)\n",
    "        latency = time.time() - start\n",
    "        \n",
    "        print(f\"â±ï¸  Latency: {latency*1000:.0f}ms\")\n",
    "        print(f\"ðŸ“„ Retrieved {len(results)} documents\\n\")\n",
    "        \n",
    "        # Display results\n",
    "        for j, doc in enumerate(results, 1):\n",
    "            source = doc.metadata.get('source', 'Unknown')\n",
    "            agent_type = doc.metadata.get('agent_type', 'Unknown')\n",
    "            \n",
    "            print(f\"{j}. Source: {source} (Agent: {agent_type})\")\n",
    "            print(f\"   Preview: {doc.page_content[:200]}...\")\n",
    "            print()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {e}\")\n",
    "        print(\"   Make sure documents are embedded for clarity agent\")\n",
    "\n",
    "print(\"\\nâœ“ Clarity RAG testing complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Rigor Agent RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TESTING RIGOR AGENT RAG\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create RAG service for rigor agent\n",
    "rigor_service = RAGService(config=rigor_rag_config, qdrant_client=qdrant_client)\n",
    "\n",
    "# Test queries for rigor\n",
    "rigor_queries = [\n",
    "    \"mathematical proof validation and structure\",\n",
    "    \"stating assumptions in theorems\",\n",
    "    \"experimental design and statistical rigor\",\n",
    "    \"checking mathematical correctness\"\n",
    "]\n",
    "\n",
    "for i, query in enumerate(rigor_queries, 1):\n",
    "    print(f\"\\n{'â”€'*60}\")\n",
    "    print(f\"Query {i}: {query}\")\n",
    "    print(f\"{'â”€'*60}\")\n",
    "    \n",
    "    try:\n",
    "        # Retrieve documents\n",
    "        start = time.time()\n",
    "        results = rigor_service.retrieve(query, top_k=3)\n",
    "        latency = time.time() - start\n",
    "        \n",
    "        print(f\"â±ï¸  Latency: {latency*1000:.0f}ms\")\n",
    "        print(f\"ðŸ“„ Retrieved {len(results)} documents\\n\")\n",
    "        \n",
    "        # Display results\n",
    "        for j, doc in enumerate(results, 1):\n",
    "            source = doc.metadata.get('source', 'Unknown')\n",
    "            agent_type = doc.metadata.get('agent_type', 'Unknown')\n",
    "            \n",
    "            print(f\"{j}. Source: {source} (Agent: {agent_type})\")\n",
    "            print(f\"   Preview: {doc.page_content[:200]}...\")\n",
    "            print()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {e}\")\n",
    "        print(\"   Make sure documents are embedded for rigor agent\")\n",
    "\n",
    "print(\"\\nâœ“ Rigor RAG testing complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test RAG Nodes (Workflow Integration)\n",
    "\n",
    "Test how RAG nodes work within the actual workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TESTING RAG NODES (WORKFLOW INTEGRATION)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create RAG nodes\n",
    "clarity_rag_node = ClarityRAGNode(qdrant_client=qdrant_client)\n",
    "rigor_rag_node = RigorRAGNode(qdrant_client=qdrant_client)\n",
    "\n",
    "# Create sample sections\n",
    "sample_sections = [\n",
    "    Section(\n",
    "        title=\"Introduction\",\n",
    "        content=\"We present a novel algorithm for optimization. The approach leverages sophisticated mathematical techniques to achieve optimal performance in various scenarios.\",\n",
    "        section_type=\"introduction\",\n",
    "        line_start=1\n",
    "    ),\n",
    "    Section(\n",
    "        title=\"Methodology\",\n",
    "        content=\"Our proof relies on the following theorem: For all x in the domain, the function f(x) converges. We assume the conditions hold without explicit validation.\",\n",
    "        section_type=\"methodology\",\n",
    "        line_start=50\n",
    "    )\n",
    "]\n",
    "\n",
    "# Test Clarity RAG Node\n",
    "print(\"\\nðŸ“˜ Testing Clarity RAG Node\")\n",
    "print(\"â”€\" * 60)\n",
    "\n",
    "clarity_state = {\n",
    "    \"sections_for_clarity\": sample_sections\n",
    "}\n",
    "\n",
    "clarity_result = clarity_rag_node(clarity_state)\n",
    "clarity_guidelines = clarity_result.get(\"clarity_guidelines\", \"\")\n",
    "\n",
    "if clarity_guidelines:\n",
    "    print(f\"âœ“ Retrieved clarity guidelines ({len(clarity_guidelines)} chars)\")\n",
    "    print(f\"\\nGuidelines Preview:\\n{clarity_guidelines[:500]}...\")\n",
    "else:\n",
    "    print(\"âš ï¸  No clarity guidelines retrieved\")\n",
    "\n",
    "# Test Rigor RAG Node\n",
    "print(\"\\n\\nðŸ“— Testing Rigor RAG Node\")\n",
    "print(\"â”€\" * 60)\n",
    "\n",
    "rigor_state = {\n",
    "    \"sections_for_rigor\": [sample_sections[1]]  # Only methodology section\n",
    "}\n",
    "\n",
    "rigor_result = rigor_rag_node(rigor_state)\n",
    "rigor_guidelines = rigor_result.get(\"rigor_guidelines\", \"\")\n",
    "\n",
    "if rigor_guidelines:\n",
    "    print(f\"âœ“ Retrieved rigor guidelines ({len(rigor_guidelines)} chars)\")\n",
    "    print(f\"\\nGuidelines Preview:\\n{rigor_guidelines[:500]}...\")\n",
    "else:\n",
    "    print(\"âš ï¸  No rigor guidelines retrieved\")\n",
    "\n",
    "print(\"\\nâœ“ RAG nodes testing complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compare Retrieval Strategies\n",
    "\n",
    "Compare naive vs other retrieval strategies (if available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARING RETRIEVAL STRATEGIES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_query = \"how to improve clarity in mathematical writing\"\n",
    "\n",
    "# Test with naive retrieval\n",
    "print(f\"\\nQuery: {test_query}\")\n",
    "print(\"\\n1ï¸âƒ£  Naive Retrieval\")\n",
    "print(\"â”€\" * 60)\n",
    "\n",
    "clarity_rag_config.retrieval_config.retriever_type = \"naive\"\n",
    "naive_service = RAGService(config=clarity_rag_config, qdrant_client=qdrant_client)\n",
    "\n",
    "start = time.time()\n",
    "naive_results = naive_service.retrieve(test_query, top_k=3)\n",
    "naive_latency = time.time() - start\n",
    "\n",
    "print(f\"â±ï¸  Latency: {naive_latency*1000:.0f}ms\")\n",
    "print(f\"ðŸ“„ Results: {len(naive_results)}\")\n",
    "for i, doc in enumerate(naive_results, 1):\n",
    "    print(f\"   {i}. {doc.metadata.get('source', 'Unknown')[:50]}\")\n",
    "\n",
    "# Try rerank if Cohere API key available\n",
    "print(\"\\n2ï¸âƒ£  Rerank Retrieval (Cohere)\")\n",
    "print(\"â”€\" * 60)\n",
    "\n",
    "try:\n",
    "    clarity_rag_config.retrieval_config.retriever_type = \"rerank\"\n",
    "    rerank_service = RAGService(config=clarity_rag_config, qdrant_client=qdrant_client)\n",
    "    \n",
    "    start = time.time()\n",
    "    rerank_results = rerank_service.retrieve(test_query, top_k=3)\n",
    "    rerank_latency = time.time() - start\n",
    "    \n",
    "    print(f\"â±ï¸  Latency: {rerank_latency*1000:.0f}ms\")\n",
    "    print(f\"ðŸ“„ Results: {len(rerank_results)}\")\n",
    "    for i, doc in enumerate(rerank_results, 1):\n",
    "        print(f\"   {i}. {doc.metadata.get('source', 'Unknown')[:50]}\")\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Comparison:\")\n",
    "    print(f\"   Naive latency: {naive_latency*1000:.0f}ms\")\n",
    "    print(f\"   Rerank latency: {rerank_latency*1000:.0f}ms\")\n",
    "    print(f\"   Overhead: {(rerank_latency/naive_latency - 1)*100:.0f}%\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  Rerank not available: {e}\")\n",
    "    print(\"   This is normal if Cohere API key is not set\")\n",
    "\n",
    "# Reset to naive\n",
    "clarity_rag_config.retrieval_config.retriever_type = \"naive\"\n",
    "\n",
    "print(\"\\nâœ“ Strategy comparison complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Interactive Query Testing\n",
    "\n",
    "Test your own custom queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose agent\n",
    "agent_choice = \"clarity\"  # or \"rigor\"\n",
    "\n",
    "# Your custom query\n",
    "custom_query = \"what makes mathematical writing clear and understandable\"\n",
    "\n",
    "print(f\"\\nðŸ” Testing custom query for {agent_choice.upper()} agent\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Query: {custom_query}\\n\")\n",
    "\n",
    "# Select service\n",
    "if agent_choice == \"clarity\":\n",
    "    service = RAGService(config=clarity_rag_config, qdrant_client=qdrant_client)\n",
    "else:\n",
    "    service = RAGService(config=rigor_rag_config, qdrant_client=qdrant_client)\n",
    "\n",
    "# Retrieve\n",
    "start = time.time()\n",
    "results = service.retrieve(custom_query, top_k=5)\n",
    "latency = time.time() - start\n",
    "\n",
    "print(f\"â±ï¸  Latency: {latency*1000:.0f}ms\")\n",
    "print(f\"ðŸ“„ Retrieved {len(results)} documents\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Display formatted results\n",
    "formatted = service.format_results(results)\n",
    "print(formatted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Diagnostic Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*60)\nprint(\"DIAGNOSTIC INFORMATION\")\nprint(\"=\"*60)\n\n# Check cache directories\nprint(\"\\nðŸ“‚ Cache Directories:\")\ncache_base = Path(\"app/embedding_cache\")\nfor agent_type in [\"clarity\", \"rigor\", \"test\"]:\n    cache_dir = cache_base / agent_type\n    if cache_dir.exists():\n        cache_files = list(cache_dir.glob(\"*\"))\n        total_size = sum(f.stat().st_size for f in cache_files if f.is_file())\n        print(f\"   {agent_type}: {len(cache_files)} files ({total_size/1024:.1f} KB)\")\n    else:\n        print(f\"   {agent_type}: Not found\")\n\n# Check Qdrant collections\nprint(\"\\nðŸ“š Qdrant Collections:\")\ntry:\n    collections = qdrant_client.get_collections()\n    for collection in collections.collections:\n        info = qdrant_client.get_collection(collection.name)\n        print(f\"   {collection.name}: {info.points_count} points\")\nexcept Exception as e:\n    print(f\"   Error: {e}\")\n\n# Configuration info\nprint(\"\\nâš™ï¸  Configuration:\")\nprint(f\"   Clarity chunk size: {clarity_rag_config.embedding_config.chunk_size}\")\nprint(f\"   Clarity top_k: {clarity_rag_config.retrieval_config.top_k}\")\nprint(f\"   Clarity retriever: {clarity_rag_config.retrieval_config.retriever_type}\")\nprint(f\"   Rigor chunk size: {rigor_rag_config.embedding_config.chunk_size}\")\nprint(f\"   Rigor top_k: {rigor_rag_config.retrieval_config.top_k}\")\nprint(f\"   Rigor retriever: {rigor_rag_config.retrieval_config.retriever_type}\")\n\n# Document resources\nprint(\"\\nðŸ“„ Document Resources:\")\nfor agent_type in [\"clarity\", \"rigor\"]:\n    docs_dir = Path(f\"app/resources/{agent_type}_docs\")\n    if docs_dir.exists():\n        pdfs = list(docs_dir.glob(\"*.pdf\"))\n        print(f\"   {agent_type}: {len(pdfs)} PDFs\")\n        for pdf in pdfs:\n            print(f\"      - {pdf.name}\")\n    else:\n        print(f\"   {agent_type}: Directory not found\")\n\nprint(\"\\nâœ“ Diagnostics complete\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook tested:\n",
    "\n",
    "1. âœ… Qdrant connection and collections\n",
    "2. âœ… Embedding cache functionality\n",
    "3. âœ… Clarity agent RAG retrieval\n",
    "4. âœ… Rigor agent RAG retrieval\n",
    "5. âœ… RAG nodes workflow integration\n",
    "6. âœ… Retrieval strategy comparison\n",
    "7. âœ… Custom query testing\n",
    "8. âœ… Diagnostic information\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- If collections are empty: `python scripts/embed_documents.py --all`\n",
    "- If cache is empty: Re-run embedding script\n",
    "- If retrieval fails: Check Qdrant is running\n",
    "- To add more documents: Add PDFs to `rag_resources/{agent}/` and re-embed\n",
    "\n",
    "### Performance Targets:\n",
    "\n",
    "- Cache speedup: >100x\n",
    "- Naive retrieval: <200ms\n",
    "- Rerank retrieval: <500ms\n",
    "- Documents per agent: >10 chunks\n",
    "\n",
    "Happy testing! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}