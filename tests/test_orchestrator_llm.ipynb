{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test LLM-Based Orchestrator\n",
    "\n",
    "This notebook tests the new LLM-based orchestrator that:\n",
    "- Detects semantic duplicates\n",
    "- Merges similar suggestions\n",
    "- Identifies contradictions\n",
    "- Processes sections in parallel\n",
    "- Filters low-value suggestions\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path(os.getcwd()).parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Set environment variable for .env file\n",
    "os.chdir(project_root)\n",
    "\n",
    "from app.agents.orchestrator_agent import OrchestratorAgent\n",
    "from app.models.schemas import (\n",
    "    SectionSuggestions,\n",
    "    SuggestionGroup,\n",
    "    SuggestionItem,\n",
    "    SuggestionType,\n",
    "    SeverityLevel,\n",
    ")\n",
    "\n",
    "print(\"✓ Imports successful\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Semantic Duplicate Detection\n",
    "\n",
    "Test if orchestrator can detect that these are duplicates:\n",
    "- \"Variable x is not defined\"\n",
    "- \"Missing definition for variable x\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test suggestions with semantic duplicates\n",
    "test_section_duplicates = SectionSuggestions(\n",
    "    section=\"Introduction\",\n",
    "    line=1,\n",
    "    section_type=\"introduction\",\n",
    "    suggestions=[\n",
    "        SuggestionGroup(\n",
    "            type=SuggestionType.CLARITY,\n",
    "            count=1,\n",
    "            items=[\n",
    "                SuggestionItem(\n",
    "                    text=\"Variable x is not defined before first use\",\n",
    "                    line=15,\n",
    "                    severity=SeverityLevel.ERROR,\n",
    "                    severity_score=0.75,\n",
    "                    explanation=\"Using undefined variables leads to confusion and potential errors.\",\n",
    "                    suggested_fix=\"Define variable x before line 15, e.g., 'Let x denote...'\"\n",
    "                )\n",
    "            ]\n",
    "        ),\n",
    "        SuggestionGroup(\n",
    "            type=SuggestionType.RIGOR,\n",
    "            count=1,\n",
    "            items=[\n",
    "                SuggestionItem(\n",
    "                    text=\"Missing definition for variable x\",\n",
    "                    line=15,\n",
    "                    severity=SeverityLevel.ERROR,\n",
    "                    severity_score=0.80,\n",
    "                    explanation=\"Mathematical rigor requires all variables to be properly defined.\",\n",
    "                    suggested_fix=\"Add formal definition: 'Let x ∈ R denote...'\"\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Before orchestration:\")\n",
    "print(f\"  Total suggestions: {sum(g.count for g in test_section_duplicates.suggestions)}\")\n",
    "for group in test_section_duplicates.suggestions:\n",
    "    for item in group.items:\n",
    "        print(f\"  - [{group.type}] {item.text} (score: {item.severity_score})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test orchestration\n",
    "orchestrator = OrchestratorAgent()\n",
    "\n",
    "async def test_duplicates():\n",
    "    result = await orchestrator.validate_and_prioritize([test_section_duplicates])\n",
    "    return result[0]\n",
    "\n",
    "result_duplicates = await test_duplicates()\n",
    "\n",
    "print(\"\\nAfter orchestration:\")\n",
    "print(f\"  Total suggestions: {sum(g.count for g in result_duplicates.suggestions)}\")\n",
    "for group in result_duplicates.suggestions:\n",
    "    for item in group.items:\n",
    "        print(f\"\\n  [{group.type}] {item.text}\")\n",
    "        print(f\"    Score: {item.severity_score:.2f}\")\n",
    "        print(f\"    Explanation: {item.explanation}\")\n",
    "        print(f\"    Fix: {item.suggested_fix}\")\n",
    "\n",
    "# Check if duplicates were merged\n",
    "total_before = sum(g.count for g in test_section_duplicates.suggestions)\n",
    "total_after = sum(g.count for g in result_duplicates.suggestions)\n",
    "\n",
    "print(f\"\\n{'✓' if total_after < total_before else '✗'} Duplicates merged: {total_before} → {total_after}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: Contradiction Detection\n",
    "\n",
    "Test if orchestrator can detect and resolve contradictions:\n",
    "- Clarity: \"This explanation is too verbose, shorten it\"\n",
    "- Rigor: \"This explanation lacks detail, expand it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test suggestions with contradiction\n",
    "test_section_contradiction = SectionSuggestions(\n",
    "    section=\"Methodology\",\n",
    "    line=20,\n",
    "    section_type=\"methodology\",\n",
    "    suggestions=[\n",
    "        SuggestionGroup(\n",
    "            type=SuggestionType.CLARITY,\n",
    "            count=1,\n",
    "            items=[\n",
    "                SuggestionItem(\n",
    "                    text=\"The methodology description is overly verbose and should be shortened\",\n",
    "                    line=25,\n",
    "                    severity=SeverityLevel.WARNING,\n",
    "                    severity_score=0.55,\n",
    "                    explanation=\"Long-winded explanations reduce readability and may confuse readers.\",\n",
    "                    suggested_fix=\"Condense the methodology section by removing redundant phrases and focusing on key steps.\"\n",
    "                )\n",
    "            ]\n",
    "        ),\n",
    "        SuggestionGroup(\n",
    "            type=SuggestionType.RIGOR,\n",
    "            count=1,\n",
    "            items=[\n",
    "                SuggestionItem(\n",
    "                    text=\"The methodology description lacks sufficient detail for reproducibility\",\n",
    "                    line=25,\n",
    "                    severity=SeverityLevel.ERROR,\n",
    "                    severity_score=0.72,\n",
    "                    explanation=\"Insufficient methodological detail makes it impossible to replicate the experiment.\",\n",
    "                    suggested_fix=\"Add specific details: dataset size, hyperparameters, training procedure, evaluation metrics.\"\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Before orchestration (contradiction):\")\n",
    "for group in test_section_contradiction.suggestions:\n",
    "    for item in group.items:\n",
    "        print(f\"  - [{group.type}] {item.text} (score: {item.severity_score})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_contradiction():\n",
    "    result = await orchestrator.validate_and_prioritize([test_section_contradiction])\n",
    "    return result[0]\n",
    "\n",
    "result_contradiction = await test_contradiction()\n",
    "\n",
    "print(\"\\nAfter orchestration (should resolve contradiction):\")\n",
    "for group in result_contradiction.suggestions:\n",
    "    for item in group.items:\n",
    "        print(f\"\\n  [{group.type}] {item.text}\")\n",
    "        print(f\"    Score: {item.severity_score:.2f}\")\n",
    "        print(f\"    Explanation: {item.explanation}\")\n",
    "        print(f\"    Fix: {item.suggested_fix}\")\n",
    "\n",
    "total_before = sum(g.count for g in test_section_contradiction.suggestions)\n",
    "total_after = sum(g.count for g in result_contradiction.suggestions)\n",
    "\n",
    "print(f\"\\nSuggestions: {total_before} → {total_after}\")\n",
    "print(\"Expected: Orchestrator should either merge into balanced suggestion or keep rigor (higher score)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3: Parallel Processing Performance\n",
    "\n",
    "Test that multiple sections are processed in parallel using asyncio.gather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Create multiple sections\n",
    "test_sections_parallel = [\n",
    "    SectionSuggestions(\n",
    "        section=f\"Section {i}\",\n",
    "        line=i * 10,\n",
    "        section_type=\"methodology\",\n",
    "        suggestions=[\n",
    "            SuggestionGroup(\n",
    "                type=SuggestionType.CLARITY,\n",
    "                count=2,\n",
    "                items=[\n",
    "                    SuggestionItem(\n",
    "                        text=f\"Issue A in section {i}\",\n",
    "                        line=i * 10 + 1,\n",
    "                        severity=SeverityLevel.WARNING,\n",
    "                        severity_score=0.60,\n",
    "                        explanation=\"Test explanation A\",\n",
    "                        suggested_fix=\"Test fix A\"\n",
    "                    ),\n",
    "                    SuggestionItem(\n",
    "                        text=f\"Issue B in section {i}\",\n",
    "                        line=i * 10 + 2,\n",
    "                        severity=SeverityLevel.INFO,\n",
    "                        severity_score=0.30,\n",
    "                        explanation=\"Test explanation B\",\n",
    "                        suggested_fix=\"Test fix B\"\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    for i in range(1, 6)  # 5 sections\n",
    "]\n",
    "\n",
    "print(f\"Testing parallel processing with {len(test_sections_parallel)} sections...\")\n",
    "\n",
    "start_time = time.time()\n",
    "result_parallel = await orchestrator.validate_and_prioritize(test_sections_parallel)\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n✓ Processed {len(result_parallel)} sections in {elapsed_time:.2f}s\")\n",
    "print(f\"  Average per section: {elapsed_time / len(result_parallel):.2f}s\")\n",
    "print(\"\\nNote: With parallel processing (asyncio.gather), total time should be ~1-2s,\")\n",
    "print(\"      not 5-10s (which would be sequential processing)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 4: Quality Control - Low Value Filtering\n",
    "\n",
    "Test if orchestrator filters out very low-value suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test with mix of high-value and low-value suggestions\n",
    "test_section_quality = SectionSuggestions(\n",
    "    section=\"Results\",\n",
    "    line=50,\n",
    "    section_type=\"results\",\n",
    "    suggestions=[\n",
    "        SuggestionGroup(\n",
    "            type=SuggestionType.CLARITY,\n",
    "            count=3,\n",
    "            items=[\n",
    "                SuggestionItem(\n",
    "                    text=\"Missing statistical significance testing for main results\",\n",
    "                    line=55,\n",
    "                    severity=SeverityLevel.ERROR,\n",
    "                    severity_score=0.85,\n",
    "                    explanation=\"Without p-values, readers cannot assess validity of claimed improvements.\",\n",
    "                    suggested_fix=\"Add t-test or ANOVA results with p-values for all claimed improvements.\"\n",
    "                ),\n",
    "                SuggestionItem(\n",
    "                    text=\"Consider using a different font for table headers\",\n",
    "                    line=60,\n",
    "                    severity=SeverityLevel.INFO,\n",
    "                    severity_score=0.10,\n",
    "                    explanation=\"Bold font might improve visual hierarchy.\",\n",
    "                    suggested_fix=\"Make table headers bold.\"\n",
    "                ),\n",
    "                SuggestionItem(\n",
    "                    text=\"The word 'significant' appears twice in one sentence\",\n",
    "                    line=58,\n",
    "                    severity=SeverityLevel.INFO,\n",
    "                    severity_score=0.15,\n",
    "                    explanation=\"Repetition of words can slightly reduce readability.\",\n",
    "                    suggested_fix=\"Replace one instance with 'substantial' or 'notable'.\"\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Before orchestration (mixed quality):\")\n",
    "for group in test_section_quality.suggestions:\n",
    "    for item in group.items:\n",
    "        print(f\"  - {item.text} (score: {item.severity_score})\")\n",
    "\n",
    "async def test_quality():\n",
    "    result = await orchestrator.validate_and_prioritize([test_section_quality])\n",
    "    return result[0]\n",
    "\n",
    "result_quality = await test_quality()\n",
    "\n",
    "print(\"\\nAfter orchestration (should filter low-value):\")\n",
    "for group in result_quality.suggestions:\n",
    "    for item in group.items:\n",
    "        print(f\"  - {item.text} (score: {item.severity_score:.2f})\")\n",
    "\n",
    "total_before = sum(g.count for g in test_section_quality.suggestions)\n",
    "total_after = sum(g.count for g in result_quality.suggestions)\n",
    "\n",
    "print(f\"\\nSuggestions: {total_before} → {total_after}\")\n",
    "print(\"Expected: Low-value suggestions (score < 0.2) should be filtered out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 5: Real-World Example - Climate Paper\n",
    "\n",
    "Test with realistic suggestions from the climate prediction paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate realistic suggestions for climate paper's Results section\n",
    "test_section_realistic = SectionSuggestions(\n",
    "    section=\"Experimental Validation\",\n",
    "    line=53,\n",
    "    section_type=\"results\",\n",
    "    suggestions=[\n",
    "        SuggestionGroup(\n",
    "            type=SuggestionType.CLARITY,\n",
    "            count=2,\n",
    "            items=[\n",
    "                SuggestionItem(\n",
    "                    text=\"The claim 'our method outperforms all baselines' is vague\",\n",
    "                    line=68,\n",
    "                    severity=SeverityLevel.WARNING,\n",
    "                    severity_score=0.65,\n",
    "                    explanation=\"Without specific metrics, readers cannot assess the magnitude of improvement.\",\n",
    "                    suggested_fix=\"Specify metrics: 'Our method achieves 15% lower MSE than the best baseline'.\"\n",
    "                ),\n",
    "                SuggestionItem(\n",
    "                    text=\"Table 1 is referenced but not included in the document\",\n",
    "                    line=68,\n",
    "                    severity=SeverityLevel.ERROR,\n",
    "                    severity_score=0.90,\n",
    "                    explanation=\"Missing tables break the flow and make results impossible to verify.\",\n",
    "                    suggested_fix=\"Add Table 1 with baseline comparisons or remove the reference.\"\n",
    "                )\n",
    "            ]\n",
    "        ),\n",
    "        SuggestionGroup(\n",
    "            type=SuggestionType.RIGOR,\n",
    "            count=2,\n",
    "            items=[\n",
    "                SuggestionItem(\n",
    "                    text=\"No statistical significance testing for claimed improvements\",\n",
    "                    line=68,\n",
    "                    severity=SeverityLevel.ERROR,\n",
    "                    severity_score=0.82,\n",
    "                    explanation=\"Without p-values or confidence intervals, improvements may not be statistically significant.\",\n",
    "                    suggested_fix=\"Add statistical tests (t-test, ANOVA) with p-values and confidence intervals.\"\n",
    "                ),\n",
    "                SuggestionItem(\n",
    "                    text=\"Missing details about baseline implementations\",\n",
    "                    line=59,\n",
    "                    severity=SeverityLevel.WARNING,\n",
    "                    severity_score=0.58,\n",
    "                    explanation=\"Fair comparison requires identical implementation details for all methods.\",\n",
    "                    suggested_fix=\"Specify: hyperparameters, training procedures, and hardware used for baselines.\"\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Realistic example - Before orchestration:\")\n",
    "print(f\"Total: {sum(g.count for g in test_section_realistic.suggestions)} suggestions\\n\")\n",
    "for group in test_section_realistic.suggestions:\n",
    "    print(f\"[{group.type}] ({group.count} suggestions):\")\n",
    "    for item in group.items:\n",
    "        print(f\"  • Line {item.line}: {item.text[:60]}...\")\n",
    "        print(f\"    Score: {item.severity_score:.2f}, Severity: {item.severity}\")\n",
    "\n",
    "async def test_realistic():\n",
    "    result = await orchestrator.validate_and_prioritize([test_section_realistic])\n",
    "    return result[0]\n",
    "\n",
    "result_realistic = await test_realistic()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"After orchestration:\")\n",
    "print(f\"Total: {sum(g.count for g in result_realistic.suggestions)} suggestions\\n\")\n",
    "for group in result_realistic.suggestions:\n",
    "    print(f\"[{group.type}] ({group.count} suggestions):\")\n",
    "    for item in group.items:\n",
    "        print(f\"\\n  • Line {item.line}: {item.text}\")\n",
    "        print(f\"    Score: {item.severity_score:.2f}, Severity: {item.severity}\")\n",
    "        print(f\"    Explanation: {item.explanation[:100]}...\")\n",
    "        print(f\"    Fix: {item.suggested_fix[:100]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Expected behavior:\")\n",
    "print(\"  • Line 68 duplicates (clarity + rigor about vague claims) should be merged\")\n",
    "print(\"  • Missing Table 1 (score 0.90) should be prioritized first\")\n",
    "print(\"  • Statistical testing (score 0.82) should be second\")\n",
    "print(\"  • Lower-score suggestions should follow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The LLM-based orchestrator should:\n",
    "1. ✓ Merge semantic duplicates (even with different wording)\n",
    "2. ✓ Detect and resolve contradictions intelligently\n",
    "3. ✓ Process sections in parallel (fast total time)\n",
    "4. ✓ Filter out very low-value suggestions\n",
    "5. ✓ Prioritize by impact × actionability, not just score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
