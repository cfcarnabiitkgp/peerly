{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peerly Architecture Testing Notebook\n",
    "\n",
    "This notebook allows you to test each component of the Peerly architecture independently.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, make sure you have:\n",
    "1. Installed all dependencies: `pip install -e .`\n",
    "2. Set up your `.env` file with `OPENAI_API_KEY`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook directory: /Users/arnabbhattacharya/Desktop/Peerly-Demo/tests\n",
      "Project root: /Users/arnabbhattacharya/Desktop/Peerly-Demo\n",
      "Examples exists: True\n",
      "App module exists: True\n",
      "Python path includes: /Users/arnabbhattacharya/Desktop/Peerly-Demo\n",
      "\n",
      "âœ… Successfully imported latex_parser!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the project root (parent of tests/ folder)\n",
    "notebook_dir = Path(__file__).parent if '__file__' in globals() else Path.cwd()\n",
    "if notebook_dir.name == 'tests':\n",
    "    project_root = notebook_dir.parent\n",
    "else:\n",
    "    # If running from project root\n",
    "    project_root = notebook_dir\n",
    "    \n",
    "# If notebook_dir doesn't have 'examples', go up one level\n",
    "if not (project_root / 'examples').exists():\n",
    "    project_root = notebook_dir.parent\n",
    "\n",
    "# Add to Python path\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Notebook directory: {notebook_dir}\")\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Examples exists: {(project_root / 'examples').exists()}\")\n",
    "print(f\"App module exists: {(project_root / 'app').exists()}\")\n",
    "print(f\"Python path includes: {sys.path[0]}\")\n",
    "\n",
    "# Verify import will work\n",
    "try:\n",
    "    from app.services.latex_parser import latex_parser\n",
    "    print(\"\\nâœ… Successfully imported latex_parser!\")\n",
    "except ImportError as e:\n",
    "    print(f\"\\nâŒ Import failed: {e}\")\n",
    "    print(f\"\\nTroubleshooting:\")\n",
    "    print(f\"  1. Current directory: {Path.cwd()}\")\n",
    "    print(f\"  2. Project root set to: {project_root}\")\n",
    "    print(f\"  3. Looking for: {project_root / 'app' / 'services' / 'latex_parser.py'}\")\n",
    "    print(f\"  4. File exists: {(project_root / 'app' / 'services' / 'latex_parser.py').exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Testing LaTeX Parser\n",
    "\n",
    "The LaTeX parser extracts sections from LaTeX documents and classifies them by type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ LaTeX parser ready to use\n"
     ]
    }
   ],
   "source": [
    "# LaTeX parser was already imported in Cell 1\n",
    "print(\"âœ“ LaTeX parser ready to use\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1.1: Simple LaTeX Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Found 4 sections:\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Section 1:\n",
      "  Title: Introduction\n",
      "  Type: introduction\n",
      "  Lines: 11 - 14\n",
      "  Content length: 76 characters\n",
      "  Content preview: \\section{Introduction}\n",
      "This is the introduction. We introduce the problem.\n",
      "\n",
      "...\n",
      "\n",
      "Section 2:\n",
      "  Title: Methodology\n",
      "  Type: methodology\n",
      "  Lines: 14 - 18\n",
      "  Content length: 114 characters\n",
      "  Content preview: \\section{Methodology}\n",
      "This section describes the methods used.\n",
      "We use a neural network with paramete...\n",
      "\n",
      "Section 3:\n",
      "  Title: Results\n",
      "  Type: results\n",
      "  Lines: 18 - 21\n",
      "  Content length: 60 characters\n",
      "  Content preview: \\section{Results}\n",
      "Here are the results of our experiments.\n",
      "\n",
      "...\n",
      "\n",
      "Section 4:\n",
      "  Title: Conclusion\n",
      "  Type: conclusion\n",
      "  Lines: 21 - 24\n",
      "  Content length: 60 characters\n",
      "  Content preview: \\section{Conclusion}\n",
      "In conclusion, our method works well.\n",
      "\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# Sample LaTeX document\n",
    "simple_latex = r\"\"\"\n",
    "\\documentclass{article}\n",
    "\\usepackage{amsmath}\n",
    "\n",
    "\\title{Test Paper}\n",
    "\\author{Test Author}\n",
    "\n",
    "\\begin{document}\n",
    "\\maketitle\n",
    "\n",
    "\\section{Introduction}\n",
    "This is the introduction. We introduce the problem.\n",
    "\n",
    "\\section{Methodology}\n",
    "This section describes the methods used.\n",
    "We use a neural network with parameters $\\theta$.\n",
    "\n",
    "\\section{Results}\n",
    "Here are the results of our experiments.\n",
    "\n",
    "\\section{Conclusion}\n",
    "In conclusion, our method works well.\n",
    "\n",
    "\\end{document}\n",
    "\"\"\"\n",
    "\n",
    "# Parse the document\n",
    "sections = latex_parser.parse_sections(simple_latex)\n",
    "\n",
    "print(f\"\\nðŸ“Š Found {len(sections)} sections:\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, section in enumerate(sections, 1):\n",
    "    print(f\"\\nSection {i}:\")\n",
    "    print(f\"  Title: {section.title}\")\n",
    "    print(f\"  Type: {section.section_type}\")\n",
    "    print(f\"  Lines: {section.line_start} - {section.line_end}\")\n",
    "    print(f\"  Content length: {len(section.content)} characters\")\n",
    "    print(f\"  Content preview: {section.content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1.2: Parse Example 1 (Climate Prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Climate Prediction Paper - Found 10 sections:\n",
      "\n",
      "================================================================================\n",
      "\n",
      "1. Introduction\n",
      "   Type: introduction\n",
      "   Lines: 19-27\n",
      "   Length: 431 chars\n",
      "   Content(start): \\section{Introduction}\n",
      "\n",
      "Climate change is an important problem. Machine learning can help predict cl...\n",
      "   Content(end): ion is challenging. Previous work has limitations. We address these limitations with our approach.\n",
      "\n",
      "...\n",
      "\n",
      "2. Methodology\n",
      "   Type: methodology\n",
      "   Lines: 27-39\n",
      "   Length: 483 chars\n",
      "   Content(start): \\section{Methodology}\n",
      "\n",
      "Our method uses a neural network with parameters $\\theta$. The loss function ...\n",
      "   Content(end): |^2$$\n",
      "\n",
      "where $\\lambda$ is a regularization parameter. We use backpropagation to compute gradients.\n",
      "\n",
      "...\n",
      "\n",
      "3. Data Preprocessing\n",
      "   Type: general\n",
      "   Lines: 39-46\n",
      "   Length: 242 chars\n",
      "   Content(start): \\subsection{Data Preprocessing}\n",
      "\n",
      "The data comes from weather stations. We normalize the inputs using...\n",
      "   Content(end): u}{\\sigma}$$\n",
      "\n",
      "Missing values are handled by interpolation. Outliers are removed using a threshold.\n",
      "\n",
      "...\n",
      "\n",
      "4. Model Architecture\n",
      "   Type: general\n",
      "   Lines: 46-53\n",
      "   Length: 233 chars\n",
      "   Content(start): \\subsection{Model Architecture}\n",
      "\n",
      "Our network has $L$ layers. Each layer applies a transformation:\n",
      "$$...\n",
      "   Content(end): } + b^{(l)})$$\n",
      "\n",
      "The activation function $\\sigma$ is ReLU. The output layer uses linear activation.\n",
      "\n",
      "...\n",
      "\n",
      "5. Experimental Validation\n",
      "   Type: results\n",
      "   Lines: 53-57\n",
      "   Length: 186 chars\n",
      "   Content(start): \\section{Experimental Validation}\n",
      "\n",
      "We evaluate our method on two datasets. The first dataset contain...\n",
      "   Content(end): ataset contains temperature readings from 100 stations. The second dataset has precipitation data.\n",
      "\n",
      "...\n",
      "\n",
      "6. Baseline Comparisons\n",
      "   Type: general\n",
      "   Lines: 57-66\n",
      "   Length: 337 chars\n",
      "   Content(start): \\subsection{Baseline Comparisons}\n",
      "\n",
      "We compare against several baselines including linear regression ...\n",
      "   Content(end): {n} (y_i - \\hat{y}_i)^2$$\n",
      "\n",
      "Our approach achieves MSE = 2.3 while the best baseline gets MSE = 4.7.\n",
      "\n",
      "...\n",
      "\n",
      "7. Results\n",
      "   Type: results\n",
      "   Lines: 66-72\n",
      "   Length: 279 chars\n",
      "   Content(start): \\subsection{Results}\n",
      "\n",
      "Table 1 shows the results. Our method performs better than existing approaches...\n",
      "   Content(end): he number of layers affects performance. More layers generally help but increase computation time.\n",
      "\n",
      "...\n",
      "\n",
      "8. Discussion\n",
      "   Type: discussion\n",
      "   Lines: 72-80\n",
      "   Length: 460 chars\n",
      "   Content(start): \\section{Discussion}\n",
      "\n",
      "Our results demonstrate that deep learning can predict climate patterns. The m...\n",
      "   Content(end): es. One direction is to use transfer learning. Another is to develop more efficient architectures.\n",
      "\n",
      "...\n",
      "\n",
      "9. Related Work\n",
      "   Type: background\n",
      "   Lines: 80-86\n",
      "   Length: 374 chars\n",
      "   Content(start): \\section{Related Work}\n",
      "\n",
      "Previous studies have used various machine learning techniques. Smith et al....\n",
      "   Content(end): er, most work focuses on specific regions. Our method is more general and can be applied globally.\n",
      "\n",
      "...\n",
      "\n",
      "10. Conclusion\n",
      "   Type: conclusion\n",
      "   Lines: 86-92\n",
      "   Length: 397 chars\n",
      "   Content(start): \\section{Conclusion}\n",
      "\n",
      "We presented a machine learning approach for climate prediction. The method us...\n",
      "   Content(end): xtend the method to other climate variables. We also plan to develop real-time prediction systems.\n",
      "\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# Read example 1\n",
    "example1_path = project_root / \"examples\" / \"example1\" / \"climate_prediction.tex\"\n",
    "\n",
    "if example1_path.exists():\n",
    "    with open(example1_path, 'r', encoding='utf-8') as f:\n",
    "        climate_latex = f.read()\n",
    "    \n",
    "    # Parse the document\n",
    "    sections = latex_parser.parse_sections(climate_latex)\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Climate Prediction Paper - Found {len(sections)} sections:\\n\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for i, section in enumerate(sections, 1):\n",
    "        print(f\"\\n{i}. {section.title}\")\n",
    "        print(f\"   Type: {section.section_type}\")\n",
    "        print(f\"   Lines: {section.line_start}-{section.line_end}\")\n",
    "        print(f\"   Length: {len(section.content)} chars\")\n",
    "        print(f\"   Content(start): {section.content[:100]}...\")\n",
    "        print(f\"   Content(end): {section.content[-100:]}...\")\n",
    "else:\n",
    "    print(\"âŒ Example 1 file not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1.3: Parse Example 2 (Quantum Computing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read example 2\n",
    "example2_path = project_root / \"examples\" / \"example2\" / \"quantum_computing.tex\"\n",
    "\n",
    "if example2_path.exists():\n",
    "    with open(example2_path, 'r', encoding='utf-8') as f:\n",
    "        quantum_latex = f.read()\n",
    "    \n",
    "    # Parse the document\n",
    "    sections = latex_parser.parse_sections(quantum_latex)\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Quantum Computing Paper - Found {len(sections)} sections:\\n\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for i, section in enumerate(sections, 1):\n",
    "        print(f\"\\n{i}. {section.title}\")\n",
    "        print(f\"   Type: {section.section_type}\")\n",
    "        print(f\"   Lines: {section.line_start}-{section.line_end}\")\n",
    "        print(f\"   Length: {len(section.content)} chars\")\n",
    "        \n",
    "        # Check if it has subsections\n",
    "        if '\\\\subsection' in section.content:\n",
    "            print(f\"   âš ï¸  Contains subsections (may need nested parsing)\")\n",
    "else:\n",
    "    print(\"âŒ Example 2 file not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1.4: Section Classification Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Testing Section Classification:\n",
      "\n",
      "============================================================\n",
      "Introduction                   â†’ introduction\n",
      "Related Work                   â†’ background\n",
      "Background                     â†’ background\n",
      "Methodology                    â†’ methodology\n",
      "Methods                        â†’ methodology\n",
      "Experimental Setup             â†’ results\n",
      "Results                        â†’ results\n",
      "Experimental Results           â†’ results\n",
      "Discussion                     â†’ discussion\n",
      "Conclusion                     â†’ conclusion\n",
      "Future Work                    â†’ general\n",
      "Some Random Section            â†’ general\n"
     ]
    }
   ],
   "source": [
    "# Test the section type classifier\n",
    "test_titles = [\n",
    "    \"Introduction\",\n",
    "    \"Related Work\",\n",
    "    \"Background\",\n",
    "    \"Methodology\",\n",
    "    \"Methods\",\n",
    "    \"Experimental Setup\",\n",
    "    \"Results\",\n",
    "    \"Experimental Results\",\n",
    "    \"Discussion\",\n",
    "    \"Conclusion\",\n",
    "    \"Future Work\",\n",
    "    \"Some Random Section\"\n",
    "]\n",
    "\n",
    "print(\"\\nðŸ” Testing Section Classification:\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for title in test_titles:\n",
    "    section_type = latex_parser._classify_section(title)\n",
    "    print(f\"{title:30} â†’ {section_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Testing Individual Agents\n",
    "\n",
    "Test each agent independently to see what suggestions they generate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2.1: Clarity Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Clarity Agent initialized\n",
      "  Agent name: Clarity Agent\n",
      "  Suggestion type: SuggestionType.CLARITY\n"
     ]
    }
   ],
   "source": [
    "from app.agents.clarity_agent import ClarityAgent\n",
    "from app.models.schemas import Section\n",
    "\n",
    "# Initialize the agent\n",
    "clarity_agent = ClarityAgent()\n",
    "\n",
    "print(\"âœ“ Clarity Agent initialized\")\n",
    "print(f\"  Agent name: {clarity_agent.agent_name}\")\n",
    "print(f\"  Suggestion type: {clarity_agent.suggestion_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Testing Clarity Agent on sample section...\n",
      "\n",
      "Section content:\n",
      "\n",
      "    Machine learning is important. It uses data to make predictions.\n",
      "    Neural networks are powerful. They work well on many tasks.\n",
      "    Our approach is novel and achieves good results.\n",
      "    \n",
      "\n",
      "================================================================================\n",
      "\n",
      "ðŸ’¡ Found 5 clarity suggestions:\n",
      "\n",
      "1. ðŸ”µ INFO\n",
      "   Line: 10\n",
      "   The statement \"Machine learning is important\" is vague; it would benefit from specifying why it is important, such as its applications or impact on various industries, to provide context and relevance to the reader.\n",
      "\n",
      "2. ðŸŸ¡ WARNING\n",
      "   Line: 10\n",
      "   The phrase \"Neural networks are powerful\" lacks specificity; it should include examples of tasks or domains where neural networks excel, which would help the reader understand their significance.\n",
      "\n",
      "3. ðŸŸ¡ WARNING\n",
      "   Line: 10\n",
      "   The term \"novel\" in \"Our approach is novel\" is undefined; it should clarify what makes the approach novel compared to existing methods to avoid ambiguity.\n",
      "\n",
      "4. ðŸ”µ INFO\n",
      "   Line: 10\n",
      "   The phrase \"achieves good results\" is subjective and lacks quantifiable metrics; providing specific results or benchmarks would enhance the credibility and clarity of the statement.\n",
      "\n",
      "5. ðŸ”µ INFO\n",
      "   Line: 10\n",
      "   The section assumes the reader has prior knowledge of machine learning and neural networks; including a brief overview of these concepts would make the content more accessible to a broader audience.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a test section with clarity issues\n",
    "test_section = Section(\n",
    "    title=\"Introduction\",\n",
    "    content=\"\"\"\n",
    "    Machine learning is important. It uses data to make predictions.\n",
    "    Neural networks are powerful. They work well on many tasks.\n",
    "    Our approach is novel and achieves good results.\n",
    "    \"\"\",\n",
    "    section_type=\"introduction\",\n",
    "    line_start=10,\n",
    "    line_end=15\n",
    ")\n",
    "\n",
    "# Review the section\n",
    "print(\"\\nðŸ” Testing Clarity Agent on sample section...\\n\")\n",
    "print(\"Section content:\")\n",
    "print(test_section.content)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# This will make an API call to OpenAI\n",
    "import asyncio\n",
    "suggestions = await clarity_agent.review_section(test_section)\n",
    "\n",
    "print(f\"\\nðŸ’¡ Found {len(suggestions)} clarity suggestions:\\n\")\n",
    "\n",
    "for i, suggestion in enumerate(suggestions, 1):\n",
    "    severity_emoji = {'info': 'ðŸ”µ', 'warning': 'ðŸŸ¡', 'error': 'ðŸ”´'}\n",
    "    emoji = severity_emoji.get(suggestion.severity, 'âšª')\n",
    "    print(f\"{i}. {emoji} {suggestion.severity.upper()}\")\n",
    "    print(f\"   Line: {suggestion.line}\")\n",
    "    print(f\"   {suggestion.text}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2.2: Rigor Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.agents.rigor_agent import RigorAgent\n",
    "\n",
    "# Initialize the agent\n",
    "rigor_agent = RigorAgent()\n",
    "\n",
    "print(\"âœ“ Rigor Agent initialized\")\n",
    "print(f\"  Agent name: {rigor_agent.agent_name}\")\n",
    "print(f\"  Suggestion type: {rigor_agent.suggestion_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test section with rigor issues\n",
    "test_section_rigor = Section(\n",
    "    title=\"Methodology\",\n",
    "    content=\"\"\"\n",
    "    We use a neural network with parameters Î¸. The loss function is:\n",
    "    L(Î¸) = Î£(y_i - f(x_i; Î¸))^2\n",
    "    \n",
    "    We train using gradient descent. The model converges quickly.\n",
    "    Our approach outperforms all baselines.\n",
    "    \"\"\",\n",
    "    section_type=\"methodology\",\n",
    "    line_start=20,\n",
    "    line_end=27\n",
    ")\n",
    "\n",
    "print(\"\\nðŸ” Testing Rigor Agent on sample section...\\n\")\n",
    "print(\"Section content:\")\n",
    "print(test_section_rigor.content)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# Review the section\n",
    "suggestions = await rigor_agent.review_section(test_section_rigor)\n",
    "\n",
    "print(f\"\\nðŸ“ Found {len(suggestions)} rigor suggestions:\\n\")\n",
    "\n",
    "for i, suggestion in enumerate(suggestions, 1):\n",
    "    severity_emoji = {'info': 'ðŸ”µ', 'warning': 'ðŸŸ¡', 'error': 'ðŸ”´'}\n",
    "    emoji = severity_emoji.get(suggestion.severity, 'âšª')\n",
    "    print(f\"{i}. {emoji} {suggestion.severity.upper()}\")\n",
    "    print(f\"   Line: {suggestion.line}\")\n",
    "    print(f\"   {suggestion.text}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2.3: Compare Both Agents on Same Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test section with both clarity and rigor issues\n",
    "combined_section = Section(\n",
    "    title=\"Results\",\n",
    "    content=\"\"\"\n",
    "    Our method performs well. The results show significant improvement.\n",
    "    We achieve better accuracy than baseline methods.\n",
    "    The MSE is 2.3 compared to 4.7 for the baseline.\n",
    "    Statistical tests confirm the improvement is significant.\n",
    "    \"\"\",\n",
    "    section_type=\"results\",\n",
    "    line_start=50,\n",
    "    line_end=55\n",
    ")\n",
    "\n",
    "print(\"\\nðŸ” Testing BOTH agents on same section...\\n\")\n",
    "print(\"Section content:\")\n",
    "print(combined_section.content)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# Get suggestions from both agents\n",
    "clarity_sugg = await clarity_agent.review_section(combined_section)\n",
    "rigor_sugg = await rigor_agent.review_section(combined_section)\n",
    "\n",
    "print(f\"\\nðŸ’¡ Clarity Agent: {len(clarity_sugg)} suggestions\")\n",
    "for i, sugg in enumerate(clarity_sugg, 1):\n",
    "    print(f\"  {i}. {sugg.text[:80]}...\")\n",
    "\n",
    "print(f\"\\nðŸ“ Rigor Agent: {len(rigor_sugg)} suggestions\")\n",
    "for i, sugg in enumerate(rigor_sugg, 1):\n",
    "    print(f\"  {i}. {sugg.text[:80]}...\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Total: {len(clarity_sugg) + len(rigor_sugg)} suggestions combined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Testing Orchestrator\n",
    "\n",
    "The orchestrator validates, deduplicates, and prioritizes suggestions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.agents.orchestrator_agent import orchestrator\n",
    "from app.models.schemas import SectionSuggestions, SuggestionGroup\n",
    "\n",
    "print(\"âœ“ Orchestrator initialized\")\n",
    "print(f\"  Agent name: {orchestrator.agent_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mock suggestions with duplicates\n",
    "from app.models.schemas import SuggestionItem, SuggestionType, SeverityLevel\n",
    "\n",
    "mock_suggestions = [\n",
    "    SectionSuggestions(\n",
    "        section=\"Introduction\",\n",
    "        line=10,\n",
    "        section_type=\"introduction\",\n",
    "        suggestions=[\n",
    "            SuggestionGroup(\n",
    "                type=SuggestionType.CLARITY,\n",
    "                count=3,\n",
    "                items=[\n",
    "                    SuggestionItem(text=\"Define machine learning\", line=10, severity=SeverityLevel.WARNING),\n",
    "                    SuggestionItem(text=\"Define machine learning for readers\", line=10, severity=SeverityLevel.WARNING),  # Duplicate\n",
    "                    SuggestionItem(text=\"Explain neural networks\", line=11, severity=SeverityLevel.INFO),\n",
    "                ]\n",
    "            ),\n",
    "            SuggestionGroup(\n",
    "                type=SuggestionType.RIGOR,\n",
    "                count=2,\n",
    "                items=[\n",
    "                    SuggestionItem(text=\"Provide experimental evidence\", line=12, severity=SeverityLevel.ERROR),\n",
    "                    SuggestionItem(text=\"Add citations\", line=13, severity=SeverityLevel.INFO),\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"\\nðŸ” Testing Orchestrator...\\n\")\n",
    "print(\"Input suggestions:\")\n",
    "for section_sugg in mock_suggestions:\n",
    "    for group in section_sugg.suggestions:\n",
    "        print(f\"\\n  {group.type.value.upper()} ({group.count} items):\")\n",
    "        for item in group.items:\n",
    "            print(f\"    - [{item.severity}] {item.text}\")\n",
    "\n",
    "# Validate and prioritize\n",
    "validated = await orchestrator.validate_and_prioritize(mock_suggestions)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nOutput after orchestration:\")\n",
    "for section_sugg in validated:\n",
    "    for group in section_sugg.suggestions:\n",
    "        print(f\"\\n  {group.type.value.upper()} ({group.count} items):\")\n",
    "        for item in group.items:\n",
    "            print(f\"    - [{item.severity}] {item.text}\")\n",
    "\n",
    "# Count changes\n",
    "original_count = sum(group.count for s in mock_suggestions for group in s.suggestions)\n",
    "final_count = sum(group.count for s in validated for group in s.suggestions)\n",
    "print(f\"\\nðŸ“Š Deduplication: {original_count} â†’ {final_count} suggestions\")\n",
    "print(f\"   Removed {original_count - final_count} duplicate(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Testing Full Workflow\n",
    "\n",
    "Test the complete LangGraph workflow end-to-end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.agents.review_workflow import review_workflow\n",
    "import time\n",
    "\n",
    "print(\"âœ“ Review workflow initialized\")\n",
    "print(\"  This will test the full LangGraph pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse a real example document\n",
    "if example1_path.exists():\n",
    "    with open(example1_path, 'r', encoding='utf-8') as f:\n",
    "        latex_content = f.read()\n",
    "    \n",
    "    print(\"\\nðŸ” Running full workflow on Example 1 (Climate Prediction)...\\n\")\n",
    "    print(\"This will:\")\n",
    "    print(\"  1. Parse LaTeX into sections\")\n",
    "    print(\"  2. Run Clarity and Rigor agents in parallel\")\n",
    "    print(\"  3. Orchestrate and validate suggestions\")\n",
    "    print(\"\\nProcessing... (this may take 10-30 seconds)\\n\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Parse sections\n",
    "    start_time = time.time()\n",
    "    sections = latex_parser.parse_sections(latex_content)\n",
    "    parse_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\nâœ“ Parsed {len(sections)} sections in {parse_time:.2f}s\")\n",
    "    \n",
    "    # Run workflow\n",
    "    workflow_start = time.time()\n",
    "    result = await review_workflow.review(sections)\n",
    "    workflow_time = time.time() - workflow_start\n",
    "    \n",
    "    print(f\"âœ“ Workflow completed in {workflow_time:.2f}s\")\n",
    "    print(f\"\\nðŸ“Š Results:\\n\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    if result['error']:\n",
    "        print(f\"âŒ Error: {result['error']}\")\n",
    "    else:\n",
    "        print(f\"\\nâœ“ Generated suggestions for {len(result['final_suggestions'])} sections:\\n\")\n",
    "        \n",
    "        total_suggestions = 0\n",
    "        clarity_count = 0\n",
    "        rigor_count = 0\n",
    "        \n",
    "        for section_sugg in result['final_suggestions']:\n",
    "            print(f\"\\nðŸ“„ {section_sugg.section}\")\n",
    "            print(f\"   Line: {section_sugg.line}\")\n",
    "            print(f\"   Type: {section_sugg.section_type}\")\n",
    "            \n",
    "            for group in section_sugg.suggestions:\n",
    "                count = len(group.items)\n",
    "                total_suggestions += count\n",
    "                \n",
    "                if group.type == SuggestionType.CLARITY:\n",
    "                    clarity_count += count\n",
    "                elif group.type == SuggestionType.RIGOR:\n",
    "                    rigor_count += count\n",
    "                \n",
    "                icon = 'ðŸ’¡' if group.type == SuggestionType.CLARITY else 'ðŸ“'\n",
    "                print(f\"   {icon} {group.type.value}: {count} suggestion(s)\")\n",
    "                \n",
    "                for item in group.items[:2]:  # Show first 2\n",
    "                    print(f\"      - [{item.severity}] {item.text[:60]}...\")\n",
    "                \n",
    "                if count > 2:\n",
    "                    print(f\"      ... and {count - 2} more\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"\\nðŸ“Š Summary:\")\n",
    "        print(f\"   Total suggestions: {total_suggestions}\")\n",
    "        print(f\"   ðŸ’¡ Clarity: {clarity_count}\")\n",
    "        print(f\"   ðŸ“ Rigor: {rigor_count}\")\n",
    "        print(f\"   â±ï¸  Total time: {parse_time + workflow_time:.2f}s\")\n",
    "        print(f\"   âš¡ Average per section: {workflow_time/len(sections):.2f}s\")\n",
    "\n",
    "else:\n",
    "    print(\"âŒ Example 1 not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Performance Analysis\n",
    "\n",
    "Analyze the performance characteristics of different components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Simulate performance metrics (replace with actual measurements)\n",
    "components = ['Parser', 'Clarity Agent', 'Rigor Agent', 'Orchestrator']\n",
    "times = [0.1, 4.5, 4.2, 0.3]  # Example times in seconds\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(components, times, color=['#3b82f6', '#a855f7', '#ef4444', '#10b981'])\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.title('Component Performance Breakdown')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, v in enumerate(times):\n",
    "    plt.text(i, v + 0.1, f'{v:.1f}s', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nðŸ“Š Performance Summary:\")\n",
    "print(f\"   Total time: {sum(times):.2f}s\")\n",
    "print(f\"   Bottleneck: {components[np.argmax(times)]} ({max(times):.2f}s)\")\n",
    "print(f\"   Fastest: {components[np.argmin(times)]} ({min(times):.2f}s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Custom Testing\n",
    "\n",
    "Test with your own LaTeX content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your own LaTeX content here\n",
    "custom_latex = r\"\"\"\n",
    "\\documentclass{article}\n",
    "\\begin{document}\n",
    "\n",
    "\\section{My Section}\n",
    "Write your LaTeX content here to test the system.\n",
    "\n",
    "\\end{document}\n",
    "\"\"\"\n",
    "\n",
    "# Parse and review\n",
    "custom_sections = latex_parser.parse_sections(custom_latex)\n",
    "print(f\"Parsed {len(custom_sections)} section(s)\")\n",
    "\n",
    "if len(custom_sections) > 0:\n",
    "    result = await review_workflow.review(custom_sections)\n",
    "    \n",
    "    if not result['error']:\n",
    "        total = sum(len(g.items) for s in result['final_suggestions'] for g in s.suggestions)\n",
    "        print(f\"\\nGenerated {total} suggestions\")\n",
    "    else:\n",
    "        print(f\"Error: {result['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook tested:\n",
    "1. âœ… LaTeX Parser - Section extraction and classification\n",
    "2. âœ… Clarity Agent - Identifying unclear writing\n",
    "3. âœ… Rigor Agent - Validating technical correctness\n",
    "4. âœ… Orchestrator - Deduplication and prioritization\n",
    "5. âœ… Full Workflow - End-to-end LangGraph pipeline\n",
    "6. âœ… Performance - Timing and bottleneck analysis\n",
    "\n",
    "Next steps:\n",
    "- Optimize slow components\n",
    "- Add more agents (Ethics, Style, Grammar)\n",
    "- Implement caching for repeated queries\n",
    "- Add RAG for technical guidelines"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
